{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#overview","title":"Overview","text":"<p>TODO</p>"},{"location":"#motivation","title":"Motivation","text":"<p>TODO</p>"},{"location":"#api-stability","title":"API stability","text":"<p> While <code>tabutorch</code> is in development stage, no API is guaranteed to be stable from one release to the next. In fact, it is very likely that the API will change multiple times before a stable 1.0.0 release. In practice, this means that upgrading <code>tabutorch</code> to a new version will possibly break any code that was using the old version of <code>tabutorch</code>.</p>"},{"location":"#license","title":"License","text":"<p><code>tabutorch</code> is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in LICENSE file.</p>"},{"location":"get_started/","title":"Get Started","text":"<p>It is highly recommended to install in a virtual environment to keep your system in order.</p>"},{"location":"get_started/#installing-with-pip-recommended","title":"Installing with <code>pip</code> (recommended)","text":"<p>The following command installs the latest version of the library:</p> <pre><code>pip install tabutorch\n</code></pre> <p>To make the package as slim as possible, only the packages required to use <code>tabutorch</code> are installed. It is possible to install all the optional dependencies by running the following command:</p> <pre><code>pip install 'tabutorch[all]'\n</code></pre> <p>This command also installed NumPy and PyTorch. It is also possible to install the optional packages manually or to select the packages to install. In the following example, only NumPy is installed:</p> <pre><code>pip install tabutorch numpy\n</code></pre>"},{"location":"get_started/#installing-from-source","title":"Installing from source","text":"<p>To install <code>tabutorch</code> from source, you can follow the steps below. First, you will need to install <code>poetry</code>. <code>poetry</code> is used to manage and install the dependencies. If <code>poetry</code> is already installed on your machine, you can skip this step. There are several ways to install <code>poetry</code> so you can use the one that you prefer. You can check the <code>poetry</code> installation by running the following command:</p> <pre><code>poetry --version\n</code></pre> <p>Then, you can clone the git repository:</p> <pre><code>git clone git@github.com:durandtibo/tabutorch.git\n</code></pre> <p>It is recommended to create a Python 3.8+ virtual environment. This step is optional so you can skip it. To create a virtual environment, you can use the following command:</p> <pre><code>make conda\n</code></pre> <p>It automatically creates a conda virtual environment. When the virtual environment is created, you can activate it with the following command:</p> <pre><code>conda activate tabutorch\n</code></pre> <p>This example uses <code>conda</code> to create a virtual environment, but you can use other tools or configurations. Then, you should install the required package to use <code>tabutorch</code> with the following command:</p> <pre><code>make install\n</code></pre> <p>This command will install all the required packages. You can also use this command to update the required packages. This command will check if there is a more recent package available and will install it. Finally, you can test the installation with the following command:</p> <pre><code>make unit-test-cov\n</code></pre>"},{"location":"refs/nan/","title":"tabutorch.nan","text":""},{"location":"refs/nan/#tabutorch.nan","title":"tabutorch.nan","text":"<p>Contain functions to manage tensor with NaN values.</p>"},{"location":"refs/nan/#tabutorch.nan.check_all_nan","title":"tabutorch.nan.check_all_nan","text":"<pre><code>check_all_nan(x: Tensor, warn: bool = False) -&gt; None\n</code></pre> <p>Check if the input tensor contains only NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The tensor to check.</p> required <code>warn</code> <code>bool</code> <p>If <code>True</code>, a warning message is raised, otherwise an error message is raised.</p> <code>False</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import check_all_nan\n&gt;&gt;&gt; check_all_nan(torch.tensor([1.0, 2.0, 3.0]))\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.check_any_nan","title":"tabutorch.nan.check_any_nan","text":"<pre><code>check_any_nan(x: Tensor, warn: bool = False) -&gt; None\n</code></pre> <p>Check if the input tensor contains at least one NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The tensor to check.</p> required <code>warn</code> <code>bool</code> <p>If <code>True</code>, a warning message is raised, otherwise an error message is raised.</p> <code>False</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import check_any_nan\n&gt;&gt;&gt; check_any_nan(torch.tensor([1.0, 2.0, 3.0]))\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.check_nan_policy","title":"tabutorch.nan.check_nan_policy","text":"<pre><code>check_nan_policy(nan_policy: str) -&gt; None\n</code></pre> <p>Check the NaN policy.</p> <p>Parameters:</p> Name Type Description Default <code>nan_policy</code> <code>str</code> <p>The NaN policy.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if <code>nan_policy</code> is not <code>'omit'</code>, <code>'propagate'</code>, or <code>'raise'</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.nan import check_nan_policy\n&gt;&gt;&gt; check_nan_policy(nan_policy=\"omit\")\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.contains_nan","title":"tabutorch.nan.contains_nan","text":"<pre><code>contains_nan(\n    tensor: Tensor,\n    nan_policy: str = \"propagate\",\n    name: str = \"input tensor\",\n) -&gt; bool\n</code></pre> <p>Indicate if the given tensor contains at least one NaN value.</p> <p>Parameters:</p> Name Type Description Default <code>tensor</code> <code>Tensor</code> <p>The tensor to check.</p> required <code>nan_policy</code> <code>str</code> <p>The NaN policy. The valid values are <code>'omit'</code>, <code>'propagate'</code>, or <code>'raise'</code>.</p> <code>'propagate'</code> <code>name</code> <code>str</code> <p>An optional name to be more precise about the tensor when the exception is raised.</p> <code>'input tensor'</code> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if the tensor contains at least one NaN value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the tensor contains at least one NaN value and <code>nan_policy</code> is <code>'raise'</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import contains_nan\n&gt;&gt;&gt; contains_nan(torch.tensor([1.0, 2.0, 3.0]))\nFalse\n&gt;&gt;&gt; contains_nan(torch.tensor([1.0, 2.0, float(\"nan\")]))\nTrue\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.mean","title":"tabutorch.nan.mean","text":"<pre><code>mean(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; Tensor\n</code></pre> <p>Return the mean values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input tensor when estimating the mean. The following options are available: <code>'omit'</code> and <code>'propagate'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Returns the mean values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>nan_policy</code> value is incorrect.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import mean\n&gt;&gt;&gt; mean(torch.tensor([1.0, 2.0, 3.0]))\ntensor(2.)\n&gt;&gt;&gt; mean(torch.tensor([1.0, 2.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; mean(torch.tensor([1.0, 2.0, float(\"nan\")]), nan_policy=\"omit\")\ntensor(1.5000)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.nanmax","title":"tabutorch.nan.nanmax","text":"<pre><code>nanmax(x: Tensor, dim: None = None) -&gt; Tensor\n</code></pre><pre><code>nanmax(\n    x: Tensor,\n    dim: int | tuple[int, ...],\n    *,\n    keepdim: bool = False\n) -&gt; max\n</code></pre> <pre><code>nanmax(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    keepdim: bool = False\n) -&gt; Tensor | max\n</code></pre> <p>Compute the maximum, while ignoring NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | max</code> <p>The maximum, while ignoring NaNs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import nanmax\n&gt;&gt;&gt; nanmax(torch.tensor([1.0, 2.0, 3.0]))\ntensor(3.)\n&gt;&gt;&gt; torch.max(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; nanmax(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(3.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.nanmin","title":"tabutorch.nan.nanmin","text":"<pre><code>nanmin(x: Tensor, dim: None = None) -&gt; Tensor\n</code></pre><pre><code>nanmin(\n    x: Tensor,\n    dim: int | tuple[int, ...],\n    *,\n    keepdim: bool = False\n) -&gt; min\n</code></pre> <pre><code>nanmin(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    keepdim: bool = False\n) -&gt; Tensor | min\n</code></pre> <p>Compute the minimum, while ignoring NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor | min</code> <p>The minimum, while ignoring NaNs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import nanmin\n&gt;&gt;&gt; nanmin(torch.tensor([1.0, 2.0, 3.0]))\ntensor(1.)\n&gt;&gt;&gt; torch.min(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; nanmin(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(1.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.nanstd","title":"tabutorch.nan.nanstd","text":"<pre><code>nanstd(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    correction: int = 1,\n    keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Compute the standard deviation, while ignoring NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>correction</code> <code>int</code> <p>The difference between the sample size and sample degrees of freedom.</p> <code>1</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The standard deviation, while ignoring NaNs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import nanstd\n&gt;&gt;&gt; nanstd(torch.tensor([1.0, 2.0, 3.0]))\ntensor(1.)\n&gt;&gt;&gt; torch.var(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; nanstd(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(1.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.nanvar","title":"tabutorch.nan.nanvar","text":"<pre><code>nanvar(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    correction: int = 1,\n    keepdim: bool = False\n) -&gt; Tensor\n</code></pre> <p>Compute the variance, while ignoring NaNs.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>correction</code> <code>int</code> <p>The difference between the sample size and sample degrees of freedom.</p> <code>1</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The variance, while ignoring NaNs.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import nanvar\n&gt;&gt;&gt; nanvar(torch.tensor([1.0, 2.0, 3.0]))\ntensor(1.)\n&gt;&gt;&gt; torch.var(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; nanvar(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(1.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.nmax","title":"tabutorch.nan.nmax","text":"<pre><code>nmax(\n    x: Tensor,\n    dim: None = None,\n    nan_policy: str = \"propagate\",\n) -&gt; Tensor\n</code></pre><pre><code>nmax(\n    x: Tensor,\n    dim: int | tuple[int, ...],\n    *,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; max\n</code></pre> <pre><code>nmax(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; Tensor | max\n</code></pre> <p>Return the maximum values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>correction</code> <p>The difference between the sample size and sample degrees of freedom.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input tensor when estimating the maximum. The following options are available: <code>'omit'</code> and <code>'propagate'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>Tensor | max</code> <p>Returns the maximum values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>nan_policy</code> value is incorrect.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import nmax\n&gt;&gt;&gt; nmax(torch.tensor([1.0, 2.0, 3.0]))\ntensor(3.)\n&gt;&gt;&gt; torch.max(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; nmax(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]), nan_policy=\"omit\")\ntensor(3.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.nmin","title":"tabutorch.nan.nmin","text":"<pre><code>nmin(\n    x: Tensor,\n    dim: None = None,\n    nan_policy: str = \"propagate\",\n) -&gt; Tensor\n</code></pre><pre><code>nmin(\n    x: Tensor,\n    dim: int | tuple[int, ...],\n    *,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; min\n</code></pre> <pre><code>nmin(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; Tensor | min\n</code></pre> <p>Return the minimum values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>correction</code> <p>The difference between the sample size and sample degrees of freedom.</p> required <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input tensor when estimating the minimum. The following options are available: <code>'omit'</code> and <code>'propagate'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>Tensor | min</code> <p>Returns the minimum values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>nan_policy</code> value is incorrect.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import nmin\n&gt;&gt;&gt; nmin(torch.tensor([1.0, 2.0, 3.0]))\ntensor(1.)\n&gt;&gt;&gt; torch.min(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; nmin(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]), nan_policy=\"omit\")\ntensor(1.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.std","title":"tabutorch.nan.std","text":"<pre><code>std(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    correction: int = 1,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; Tensor\n</code></pre> <p>Return the standard deviation values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>correction</code> <code>int</code> <p>The difference between the sample size and sample degrees of freedom.</p> <code>1</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input tensor when estimating the standard deviation. The following options are available: <code>'omit'</code> and <code>'propagate'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Returns the standard deviation values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>nan_policy</code> value is incorrect.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import std\n&gt;&gt;&gt; std(torch.tensor([1.0, 2.0, 3.0]))\ntensor(1.)\n&gt;&gt;&gt; std(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; std(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]), nan_policy=\"omit\")\ntensor(1.)\n</code></pre>"},{"location":"refs/nan/#tabutorch.nan.var","title":"tabutorch.nan.var","text":"<pre><code>var(\n    x: Tensor,\n    dim: int | tuple[int, ...] | None = None,\n    *,\n    correction: int = 1,\n    keepdim: bool = False,\n    nan_policy: str = \"propagate\"\n) -&gt; Tensor\n</code></pre> <p>Return the variance values.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor.</p> required <code>dim</code> <code>int | tuple[int, ...] | None</code> <p>The dimension or dimensions to reduce. If <code>None</code>, all dimensions are reduced.</p> <code>None</code> <code>correction</code> <code>int</code> <p>The difference between the sample size and sample degrees of freedom.</p> <code>1</code> <code>keepdim</code> <code>bool</code> <p>Whether the output tensor has dim retained or not.</p> <code>False</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input tensor when estimating the variance. The following options are available: <code>'omit'</code> and <code>'propagate'</code>.</p> <code>'propagate'</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Returns the variance values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>nan_policy</code> value is incorrect.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.nan import var\n&gt;&gt;&gt; var(torch.tensor([1.0, 2.0, 3.0]))\ntensor(1.)\n&gt;&gt;&gt; var(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]))\ntensor(nan)\n&gt;&gt;&gt; var(torch.tensor([1.0, 2.0, 3.0, float(\"nan\")]), nan_policy=\"omit\")\ntensor(1.)\n</code></pre>"},{"location":"refs/preprocessing/","title":"tabutorch.preprocessing","text":""},{"location":"refs/preprocessing/#tabutorch.preprocessing","title":"tabutorch.preprocessing","text":"<p>Contain modules to prepare or transform data.</p>"},{"location":"refs/preprocessing/#tabutorch.preprocessing.BaseTransformer","title":"tabutorch.preprocessing.BaseTransformer","text":"<p>               Bases: <code>Generic[T]</code>, <code>Module</code></p> <p>Define the base class to implement a data transformer.</p>"},{"location":"refs/preprocessing/#tabutorch.preprocessing.StandardScaler","title":"tabutorch.preprocessing.StandardScaler","text":"<p>               Bases: <code>BaseTransformer[Tensor]</code></p> <p>Standardize features by removing the mean and scaling to unit variance.</p> <p>The standard score of a sample <code>x</code> is calculated as: <code>z = (x - m) / s</code> where <code>m</code> is the mean of the training samples, and <code>s</code> is the standard deviation of the training samples. The mean and standard deviation are computed when <code>fit</code> or <code>fit_transform</code> are called. Calling <code>forward</code> does not update the mean and standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>num_features</code> <code>int</code> <p>The number of features or channels of the input.</p> required <code>std_correction</code> <code>int</code> <p>The difference between the sample size and sample degrees of freedom when estimating the standard deviation.</p> <code>1</code> <code>nan_policy</code> <code>str</code> <p>The policy on how to handle NaN values in the input tensor when estimating the mean and standard deviation. The following options are available: <code>'omit'</code>, <code>'propagate'</code>, and <code>'raise'</code>.</p> <code>'propagate'</code> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; import torch\n&gt;&gt;&gt; from tabutorch.preprocessing import StandardScaler\n&gt;&gt;&gt; x = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0], [9.0, 10.0, 11.0]])\n&gt;&gt;&gt; module = StandardScaler(num_features=3)\n&gt;&gt;&gt; module\nStandardScaler(num_features=3, std_correction=1, nan_policy='propagate')\n&gt;&gt;&gt; module.fit(x)\n&gt;&gt;&gt; module.transform(x)\ntensor([[-1.1619, -1.1619, -1.1619],\n        [-0.3873, -0.3873, -0.3873],\n        [ 0.3873,  0.3873,  0.3873],\n        [ 1.1619,  1.1619,  1.1619]])\n</code></pre> Note <p><code>std_correction</code> needs to be set to <code>0</code> to be equivalent     to <code>sklearn.preprocessing.StandardScaler</code>.</p>"},{"location":"refs/root/","title":"tabutorch","text":""},{"location":"refs/root/#tabutorch","title":"tabutorch","text":"<p>Root package.</p>"},{"location":"refs/utils/","title":"tabutorch.utils","text":""},{"location":"refs/utils/#tabutorch.utils.imports","title":"tabutorch.utils.imports","text":"<p>Implement some utility functions to manage optional dependencies.</p>"},{"location":"refs/utils/#tabutorch.utils.imports.check_objectory","title":"tabutorch.utils.imports.check_objectory","text":"<pre><code>check_objectory() -&gt; None\n</code></pre> <p>Check if the <code>objectory</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>objectory</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.utils.imports import check_objectory\n&gt;&gt;&gt; check_objectory()\n</code></pre>"},{"location":"refs/utils/#tabutorch.utils.imports.check_sklearn","title":"tabutorch.utils.imports.check_sklearn","text":"<pre><code>check_sklearn() -&gt; None\n</code></pre> <p>Check if the <code>sklearn</code> package is installed.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>sklearn</code> package is not installed.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.utils.imports import check_sklearn\n&gt;&gt;&gt; check_sklearn()\n</code></pre>"},{"location":"refs/utils/#tabutorch.utils.imports.is_objectory_available","title":"tabutorch.utils.imports.is_objectory_available","text":"<pre><code>is_objectory_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>objectory</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>objectory</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.utils.imports import is_objectory_available\n&gt;&gt;&gt; is_objectory_available()\n</code></pre>"},{"location":"refs/utils/#tabutorch.utils.imports.is_sklearn_available","title":"tabutorch.utils.imports.is_sklearn_available","text":"<pre><code>is_sklearn_available() -&gt; bool\n</code></pre> <p>Indicate if the <code>sklearn</code> package is installed or not.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> if <code>sklearn</code> is available otherwise <code>False</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.utils.imports import is_sklearn_available\n&gt;&gt;&gt; is_sklearn_available()\n</code></pre>"},{"location":"refs/utils/#tabutorch.utils.imports.objectory_available","title":"tabutorch.utils.imports.objectory_available","text":"<pre><code>objectory_available(\n    fn: Callable[..., Any],\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>objectory</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>objectory</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.utils.imports import objectory_available\n&gt;&gt;&gt; @objectory_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"},{"location":"refs/utils/#tabutorch.utils.imports.sklearn_available","title":"tabutorch.utils.imports.sklearn_available","text":"<pre><code>sklearn_available(\n    fn: Callable[..., Any],\n) -&gt; Callable[..., Any]\n</code></pre> <p>Implement a decorator to execute a function only if <code>sklearn</code> package is installed.</p> <p>Parameters:</p> Name Type Description Default <code>fn</code> <code>Callable[..., Any]</code> <p>The function to execute.</p> required <p>Returns:</p> Type Description <code>Callable[..., Any]</code> <p>A wrapper around <code>fn</code> if <code>sklearn</code> package is installed, otherwise <code>None</code>.</p> <p>Example usage:</p> <pre><code>&gt;&gt;&gt; from tabutorch.utils.imports import sklearn_available\n&gt;&gt;&gt; @sklearn_available\n... def my_function(n: int = 0) -&gt; int:\n...     return 42 + n\n...\n&gt;&gt;&gt; my_function()\n</code></pre>"}]}